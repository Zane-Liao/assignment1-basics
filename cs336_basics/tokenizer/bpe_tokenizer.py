
from tokenizer import Tokenizer, merge

# @dataclass(frozen=True)
class BPETokenizerParams:
    """All you need to specify a BPETokenizer."""
    vocab: dict[int, bytes]     # index -> bytes
    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index

class BPETokenizer(Tokenizer):
    """BPE tokenizer given a set of merges and a vocabulary."""
    def __init__(self, params: BPETokenizerParams):
        self.params = params
    def encode(self, string: str) -> list[int]:
        indices = list(map(int, string.encode("utf-8")))  # @inspect indices
        # Note: this is a very slow implementation
        for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index
            indices = merge(indices, pair, new_index)
        return indices

    def decode(self, indices: list[int]) -> str:
        bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list
        string = b"".join(bytes_list).decode("utf-8")  # @inspect string
        return string


def bpe_tokenizer():
    raise NotImplementedError

def bpe_train(
    input_path: str, 
    vocab_size: int, 
    speical_token: list[str],
    vocab: dict[int, bytes],
    merges: list[tuple[bytes, bytes]]
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """"""
    raise NotImplementedError